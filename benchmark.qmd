---
title: "Benchmark"
toc: true
toc-location: left
toc-depth: 6
---

## Strategies 

### Tool-based strategies 

#### 1. ddqc
**Article**: [https://doi.org/10.1186/s13059-022-02820-w](https://doi.org/10.1186/s13059-022-02820-w)

[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/ayshwaryas/ddqc) [![GitHub stars](https://img.shields.io/github/stars/ayshwaryas/ddqc?style=social)](https://github.com/ayshwaryas/ddqc) ![Last Commit](https://img.shields.io/github/last-commit/ayshwaryas/ddqc)

Identifies damaged cells within sample-specific clusters using the Louvain algorithm for clustering and MAD-based univariate outlier detection of UMI, feature counts, and mitochondrial or ribosomal percentages.

[Tutorial](https://github.com/ayshwaryas/ddqc/blob/master/tutorials/ddqc_tutorial.ipynb)


#### 2. DropletQC
**Article**: [https://doi.org/10.1186/s13059-021-02547-0](https://doi.org/10.1186/s13059-021-02547-0)

[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/powellgenomicslab/DropletQC) [![GitHub stars](https://img.shields.io/github/stars/powellgenomicslab/DropletQC?style=social)](https://github.com/powellgenomicslab/DropletQC) ![Last Commit](https://img.shields.io/github/last-commit/powellgenomicslab/DropletQC)

Uses nuclear fraction (ratio of unspliced to spliced transcripts) and UMI counts with an Expectation-Maximization model to classify damaged cells.

[Tutorial](https://powellgenomicslab.github.io/DropletQC/articles/DropletQC.html#identifying-damaged-cells-1)


#### 3. EnsembleKQC 
**Article**: [https://doi.org/10.1007/978-3-030-26969-2_47](https://doi.org/10.1007/978-3-030-26969-2_47)

[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/mzhq/EnsembleKQC) [![GitHub stars](https://img.shields.io/github/stars/mzhq/EnsembleKQC?style=social)](https://github.com/mzhq/EnsembleKQC) ![Last Commit](https://img.shields.io/github/last-commit/mzhq/EnsembleKQC)

Combines multiple quality control measures to detect damaged cells, focusing on ensemble-based metrics 


#### 4. miQC 
**Article**: [https://doi.org/10.1371/journal.pcbi.1009290](https://doi.org/10.1371/journal.pcbi.1009290)

[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/greenelab/miQC) [![GitHub stars](https://img.shields.io/github/stars/greenelab/miQC?style=social)](https://github.com/greenelab/miQC) ![Last Commit](https://img.shields.io/github/last-commit/greenelab/miQC)

Applies a mixture model to mitochondrial percentage and library complexity to assign posterior probabilities for identifying damaged cells.

[Tutorial](https://www.bioconductor.org/packages/release/bioc/vignettes/miQC/inst/doc/miQC.html)


#### 5. SampleQC 
**Article**: [https://doi.org/10.1186/s13059-023-02859-3](https://doi.org/10.1186/s13059-023-02859-3)

[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/wmacnair/SampleQC) [![GitHub stars](https://img.shields.io/github/stars/wmacnair/SampleQC?style=social)](https://github.com/wmacnair/SampleQC) ![Last Commit](https://img.shields.io/github/last-commit/wmacnair/SampleQC)

Automates sample-wide multivariate outlier detection using robust Mahalanobis distances for mitochondrial, ribosomal, and UMI counts.

[Tutorial](https://github.com/wmacnair/SampleQC/blob/main/README.md)


#### 6. Scater 
**Article**: [https://doi.org/10.1093/bioinformatics/btw777](https://doi.org/10.1093/bioinformatics/btw777)

[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/alanocallaghan/scater) [![GitHub stars](https://img.shields.io/github/stars/alanocallaghan/scater?style=social)](https://github.com/alanocallaghan/scater) ![Last Commit](https://img.shields.io/github/last-commit/alanocallaghan/scater)

Provides diagnostic visualizations and filtering functions to assess and remove damaged cells using user-defined thresholds

[Tutorial](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-scater-qc/tutorial.html#automatic-pca-filtering)

#### 7. valiDrops
**Article**: [https://doi.org/10.1101/2023.02.07.526574](https://doi.org/10.1101/2023.02.07.526574)

[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/madsen-lab/valiDrops)  
[![GitHub stars](https://img.shields.io/github/stars/madsen-lab/valiDrops?style=social)](https://github.com/madsen-lab/valiDrops) ![Last Commit](https://img.shields.io/github/last-commit/madsen-lab/valiDrops)

Detects apoptotic cells using scaled and transformed QC features with logistic regression and cross-validation for soft labeling.

*******

### Manual strategies 

#### Fixed thresholding 

##### 8. Mitochondrial percent expression
Involves setting a fixed value for mitochondrial percent expression above which cells are classified as damaged. Implementation of this method varies based on preferred platform, for example using default functions in the [```Seurat```](https://satijalab.org/seurat/) R package,


```{r, eval=FALSE}
data[["percent.mt"]] <- PercentageFeatureSet(data, pattern = "^MT-")
data <- subset(data, subset = percent.mt < 10)

```


And the [```Scanpy```](https://scanpy.readthedocs.io/en/stable/) python package, 


```{python, eval=FALSE}
data.var['mt'] = data.var_names.str.startswith('MT-')
sc.pp.calculate_qc_metrics(data, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)
data = data[data.obs.pct_counts_mt < 10, :]
```

<br>

#### Adaptive thresholding 
Thresholds set based on the dataset's distribution of a certain metric using a simple percentile-based method. Upper thresholds (e.g., for mitochondrial and MALAT1 percent expression) were set at the 90th percentile, while lower thresholds (e.g., for ribosomal percent expression and library size) were set at the 10th percentile.
Note: library size refers to an equally weighted consideration of the log10 transformed UMI and feature counts. 

##### 9. Mitochondrial percent expression 
Filtering cells with 

- mitochondrial percent expression above the 90th percentile. 

##### 10. Mitochondrial & ribosomal percent expression 
Filtering cells with 

- mitochondrial percent expression above the 90th percentile AND 
- with ribosomal percent expression below the 10th percentile.

##### 11. Mitochondrial, ribosomal percent expression & library size 
Filtering cells with 

- mitochondrial percent expression above the 90th percentile, 
- ribosomal percent expression below the 10th percentile AND 
- library size below the 10th percentile. 

##### 12. Library size 
Filtering cells with 

- library sizes below the 10th percentile.

##### 13. MALAT1 percent expression 
Filtering cells with 

- MALAT1 percent expression above the 90th percentile. 

##### 14. MALAT1 & mitochondrial percent expression 
Filtering cells with 

- MALAT1 percent expression above the 90th percentile AND 
- mitochondrial percent expression above the 90th percentile. 

<br>
<br>

*******

## Metrics

### Performance metrics 

$$
\text{Performance} = \text{Precision} + \text{PR-AUC} + \text{Jaccard Index} + \text{F1 Score}
$$

where, 

- **Precision**  $\text{(0, 1)}$ Of all damaged predictions made, what proportion were true damaged cells?  
  _Median score taken from the 5 experimentally verified ground truth datasets._

- **PR-AUC**  $\text{(0, 1)}$ Of all predictions, were damaged cells correctly and comprehensively identified (i.e., were true damaged cells missed)?  
  _Median score taken from the 5 experimentally verified ground truth datasets._

- **Jaccard Index** $\text{(0, 1)}$  How similar were the filtered datasets to the positive, damage-free control (closer to the control indicates better correction for damage)?  
  _Median score taken from the 30 simulated ground truth datasets._

- **F1 Score** $\text{(0, 1)}$ Were the differentially expressed genes of the filtered datasets correct compared to the positive, damage-free control?  
  _Median score taken from the 30 simulated ground truth datasets._

Thus, **Performance** lies in the range   $\text{(0, 4)}$, with 4 being the highest performance possible.

You can view the individual performance scores for the strategies, [here](). 

*******

### Performance agnostic metrics

#### Damage focus 
This score measures how many of the damaged predictions originate from the population of damaged cells that have a heavily altered transcriptomic profile compared to control cells. Here, a strategy that is highly specific to this population, rather than to the population of damaged cells that appear identical to control cells and pose no contamination risk, is ideal. This value, derived from the median across the 5 experimentally verified ground truth datasets, is a proportion and lies in the range  $\text{(0, 1)}$ where we suggest that closer to 1 is ideal.

You can view the isolated damage focus scores, [here](damage-focus.html). 

<br>


#### Stringency 
This score measures how many of the cells present were labelled as damaged by the strategy. While this percentage is not especially valuable when viewed in an isolated dataset, collecting it across many datasets, here 51, gives a good idea of how stringent the strategy is in general and how a user can expect it to be when applied to new data. This is offered in response to one of the most prominent concerns for damaged cell filtering strategies: that they will remove too many cells, i.e. be too stringent, or won't remove enough, i.e. be too lenient. Being a percentage, this score ranges from $\text{(0, 100)}$ where closer to 100 represents more stringent strategies.

You can view the isolated stringency scores, [here](stringency.html). 

<br>

#### Consistency 
This score measures the deviation in the output of a tool across datasets,  

$$
\text{Consistency} = 1 - \left( \text{c}_1 \cdot \text{similarity deviation} + \text{c}_2 \cdot \text{stringency deviation} \right)
$$



made up of two components, $\text{similarity deviation}$ and $\text{stringency deviation}$, combined by constants $c_{1}$ and $c_{2}$ which are both $0.5$ but can be altered to emphasize's ones contribution over the other. 


The $\text{similarity deviation}$ for a given strategy $i$ is computed as the mean of the standard deviations of Cohen's kappa scores $\kappa_{ij}$ for strategy $i$ compared to another strategy $j$ $(j \neq i)$ across datasets. Here, $\kappa_{ij}$ is calculated by comparing the set of damaged cell labels given by strategy $i$ to those given by strategy $j$ for the same dataset. The standard deviation $\sigma(\kappa_{ij})$ is taken across all datasets, and the mean is calculated over the all deviation scores for tool $i$, amounting to ${(J - 1)}$ where $J$ is the total number of strategies. Since $\kappa$ ranges from $-1$ to $1$, where $1$ indicates complete agreement and $-1$ indicates complete disagreement, deviation scores typically range on the lower end of $(0, 1)$. To standardize the similarity deviation score for the consistency equation, min-max scaling is applied across strategies, setting the highest mean deviation to $1$ and the lowest to $0$.


$$
 \text{similarity deviation} = \frac{1}{J - 1} \sum_{j \neq i} \sigma(\kappa_{ij})
$$

More simply, $\text{stringency deviation}$ is taken as the standard deviation in the stringency score for a particular strategy, $i$, across datasets, where stringency is the percentage of damaged cells detected by the strategy. And as with $\text{similarity deviation}$, the $\text{stringency deviation}$ score is standardised by min-max scaling for the consistency equation, setting the highest deviation to $1$ and the lowest to $0$.

$$
 \text{stringency deviation} =  \sigma(\text{damaged cells / total cells})
$$ 


<br>

Note, each component is measured across the dataset types in isolation before being combined in the form of a median to generate the final deviation score. i.e, $\text{stringency deviation}$ is the median of the score taken across the groundtruth, non-groundtruth, and simulated datasets. This is done to avoid imbalancing the consistency score with any one dataset type. 

<br>

You can view the isolated consistency scores, [here](consistency.html). 

<br>
<br>

#### Usability 
This is a manual and automated scoring of tool-based strategies according to 6 criteria, 

| Criterion        | Automated |                       | No  | Yes  | Maximum |
|----------------------------------|-------------|-----|------|---------|
| Presence of documentation                     |  | 0   | 1    | 1       |
| More than one code deployment                 | ✓ | 0   | 1    | 1       |
| Last adjustment to tool less than two years ago | ✓ | 0   | 1    | 1       |
| Alterations to core code required             |  | 1   | 0    | 1       |
| GitHub stars[^1]                              | ✓ | 0   | ...  | 5       |
| Ease of use[^2]                               |  | ... | ...  | 5       |

[^1]: GitHub stars underwent min-max scaling.  
[^2]: Subjective score based on installation, dependencies, number of steps required to obtain output classification, accessibility of input data information, etc.

You can view the isolated usability scores, [here](usability.html). 

<br>

*******

To add a new metric or suggest an adjustment to the above metrics, please visit the [Contribute](contribute.html) page. 

*******
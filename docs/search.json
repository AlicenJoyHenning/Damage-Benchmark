[
  {
    "objectID": "usability.html",
    "href": "usability.html",
    "title": "Usability",
    "section": "",
    "text": "Explore the current usability results below or return to consensus."
  },
  {
    "objectID": "damage-focus.html",
    "href": "damage-focus.html",
    "title": "Damage focus",
    "section": "",
    "text": "Explore the current damage focus results below or return to consensus."
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "Benchmark",
    "section": "",
    "text": "Article: https://doi.org/10.1186/s13059-022-02820-w\n  \nIdentifies damaged cells within sample-specific clusters using the Louvain algorithm for clustering and MAD-based univariate outlier detection of UMI, feature counts, and mitochondrial or ribosomal percentages.\nTutorial\n\n\n\nArticle: https://doi.org/10.1186/s13059-021-02547-0\n  \nUses nuclear fraction (ratio of unspliced to spliced transcripts) and UMI counts with an Expectation-Maximization model to classify damaged cells.\nTutorial\n\n\n\nArticle: https://doi.org/10.1007/978-3-030-26969-2_47\n  \nCombines multiple quality control measures to detect damaged cells, focusing on ensemble-based metrics\n\n\n\nArticle: https://doi.org/10.1371/journal.pcbi.1009290\n  \nApplies a mixture model to mitochondrial percentage and library complexity to assign posterior probabilities for identifying damaged cells.\nTutorial\n\n\n\nArticle: https://doi.org/10.1186/s13059-023-02859-3\n  \nAutomates sample-wide multivariate outlier detection using robust Mahalanobis distances for mitochondrial, ribosomal, and UMI counts.\nTutorial\n\n\n\nArticle: https://doi.org/10.1093/bioinformatics/btw777\n  \nProvides diagnostic visualizations and filtering functions to assess and remove damaged cells using user-defined thresholds\nTutorial\n\n\n\nArticle: https://doi.org/10.1101/2023.02.07.526574\n\n \nDetects apoptotic cells using scaled and transformed QC features with logistic regression and cross-validation for soft labeling.\n\n\n\n\n\n\n\n\n\nInvolves setting a fixed value for mitochondrial percent expression above which cells are classified as damaged. Implementation of this method varies based on preferred platform, for example using default functions in the Seurat R package,\n\ndata[[\"percent.mt\"]] &lt;- PercentageFeatureSet(data, pattern = \"^MT-\")\ndata &lt;- subset(data, subset = percent.mt &lt; 10)\n\nAnd the Scanpy python package,\n\ndata.var['mt'] = data.var_names.str.startswith('MT-')\nsc.pp.calculate_qc_metrics(data, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\ndata = data[data.obs.pct_counts_mt &lt; 10, :]\n\n\n\n\n\n\nThresholds set based on the dataset’s distribution of a certain metric using a simple percentile-based method. Upper thresholds (e.g., for mitochondrial and MALAT1 percent expression) were set at the 90th percentile, while lower thresholds (e.g., for ribosomal percent expression and library size) were set at the 10th percentile. Note: library size refers to an equally weighted consideration of the log10 transformed UMI and feature counts.\n\n\nFiltering cells with\n\nmitochondrial percent expression above the 90th percentile.\n\n\n\n\nFiltering cells with\n\nmitochondrial percent expression above the 90th percentile AND\nwith ribosomal percent expression below the 10th percentile.\n\n\n\n\nFiltering cells with\n\nmitochondrial percent expression above the 90th percentile,\nribosomal percent expression below the 10th percentile AND\nlibrary size below the 10th percentile.\n\n\n\n\nFiltering cells with\n\nlibrary sizes below the 10th percentile.\n\n\n\n\nFiltering cells with\n\nMALAT1 percent expression above the 90th percentile.\n\n\n\n\nFiltering cells with\n\nMALAT1 percent expression above the 90th percentile AND\nmitochondrial percent expression above the 90th percentile."
  },
  {
    "objectID": "benchmark.html#strategies",
    "href": "benchmark.html#strategies",
    "title": "Benchmark",
    "section": "",
    "text": "Article: https://doi.org/10.1186/s13059-022-02820-w\n  \nIdentifies damaged cells within sample-specific clusters using the Louvain algorithm for clustering and MAD-based univariate outlier detection of UMI, feature counts, and mitochondrial or ribosomal percentages.\nTutorial\n\n\n\nArticle: https://doi.org/10.1186/s13059-021-02547-0\n  \nUses nuclear fraction (ratio of unspliced to spliced transcripts) and UMI counts with an Expectation-Maximization model to classify damaged cells.\nTutorial\n\n\n\nArticle: https://doi.org/10.1007/978-3-030-26969-2_47\n  \nCombines multiple quality control measures to detect damaged cells, focusing on ensemble-based metrics\n\n\n\nArticle: https://doi.org/10.1371/journal.pcbi.1009290\n  \nApplies a mixture model to mitochondrial percentage and library complexity to assign posterior probabilities for identifying damaged cells.\nTutorial\n\n\n\nArticle: https://doi.org/10.1186/s13059-023-02859-3\n  \nAutomates sample-wide multivariate outlier detection using robust Mahalanobis distances for mitochondrial, ribosomal, and UMI counts.\nTutorial\n\n\n\nArticle: https://doi.org/10.1093/bioinformatics/btw777\n  \nProvides diagnostic visualizations and filtering functions to assess and remove damaged cells using user-defined thresholds\nTutorial\n\n\n\nArticle: https://doi.org/10.1101/2023.02.07.526574\n\n \nDetects apoptotic cells using scaled and transformed QC features with logistic regression and cross-validation for soft labeling.\n\n\n\n\n\n\n\n\n\nInvolves setting a fixed value for mitochondrial percent expression above which cells are classified as damaged. Implementation of this method varies based on preferred platform, for example using default functions in the Seurat R package,\n\ndata[[\"percent.mt\"]] &lt;- PercentageFeatureSet(data, pattern = \"^MT-\")\ndata &lt;- subset(data, subset = percent.mt &lt; 10)\n\nAnd the Scanpy python package,\n\ndata.var['mt'] = data.var_names.str.startswith('MT-')\nsc.pp.calculate_qc_metrics(data, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\ndata = data[data.obs.pct_counts_mt &lt; 10, :]\n\n\n\n\n\n\nThresholds set based on the dataset’s distribution of a certain metric using a simple percentile-based method. Upper thresholds (e.g., for mitochondrial and MALAT1 percent expression) were set at the 90th percentile, while lower thresholds (e.g., for ribosomal percent expression and library size) were set at the 10th percentile. Note: library size refers to an equally weighted consideration of the log10 transformed UMI and feature counts.\n\n\nFiltering cells with\n\nmitochondrial percent expression above the 90th percentile.\n\n\n\n\nFiltering cells with\n\nmitochondrial percent expression above the 90th percentile AND\nwith ribosomal percent expression below the 10th percentile.\n\n\n\n\nFiltering cells with\n\nmitochondrial percent expression above the 90th percentile,\nribosomal percent expression below the 10th percentile AND\nlibrary size below the 10th percentile.\n\n\n\n\nFiltering cells with\n\nlibrary sizes below the 10th percentile.\n\n\n\n\nFiltering cells with\n\nMALAT1 percent expression above the 90th percentile.\n\n\n\n\nFiltering cells with\n\nMALAT1 percent expression above the 90th percentile AND\nmitochondrial percent expression above the 90th percentile."
  },
  {
    "objectID": "benchmark.html#metrics",
    "href": "benchmark.html#metrics",
    "title": "Benchmark",
    "section": "Metrics",
    "text": "Metrics\n\nPerformance metrics\n\\[\n\\text{Performance} = \\text{Precision} + \\text{PR-AUC} + \\text{Jaccard Index} + \\text{F1 Score}\n\\]\nwhere,\n\nPrecision \\(\\text{(0, 1)}\\) Of all damaged predictions made, what proportion were true damaged cells?\nMedian score taken from the 5 experimentally verified ground truth datasets.\nPR-AUC \\(\\text{(0, 1)}\\) Of all predictions, were damaged cells correctly and comprehensively identified (i.e., were true damaged cells missed)?\nMedian score taken from the 5 experimentally verified ground truth datasets.\nJaccard Index \\(\\text{(0, 1)}\\) How similar were the filtered datasets to the positive, damage-free control (closer to the control indicates better correction for damage)?\nMedian score taken from the 30 simulated ground truth datasets.\nF1 Score \\(\\text{(0, 1)}\\) Were the differentially expressed genes of the filtered datasets correct compared to the positive, damage-free control?\nMedian score taken from the 30 simulated ground truth datasets.\n\nThus, Performance lies in the range \\(\\text{(0, 4)}\\), with 4 being the highest performance possible.\nYou can view the individual performance scores for the strategies, here.\n\n\n\nPerformance agnostic metrics\n\nDamage focus\nThis score measures how many of the damaged predictions originate from the population of damaged cells that have a heavily altered transcriptomic profile compared to control cells. Here, a strategy that is highly specific to this population, rather than to the population of damaged cells that appear identical to control cells and pose no contamination risk, is ideal. This value, derived from the median across the 5 experimentally verified ground truth datasets, is a proportion and lies in the range \\(\\text{(0, 1)}\\) where we suggest that closer to 1 is ideal.\nYou can view the isolated damage focus scores, here.\n\n\n\nStringency\nThis score measures how many of the cells present were labelled as damaged by the strategy. While this percentage is not especially valuable when viewed in an isolated dataset, collecting it across many datasets, here 51, gives a good idea of how stringent the strategy is in general and how a user can expect it to be when applied to new data. This is offered in response to one of the most prominent concerns for damaged cell filtering strategies: that they will remove too many cells, i.e. be too stringent, or won’t remove enough, i.e. be too lenient. Being a percentage, this score ranges from \\(\\text{(0, 100)}\\) where closer to 100 represents more stringent strategies.\nYou can view the isolated stringency scores, here.\n\n\n\nConsistency\nThis score measures the deviation in the output of a tool across datasets,\n\\[\n\\text{Consistency} = 1 - \\left( \\text{c}_1 \\cdot \\text{similarity deviation} + \\text{c}_2 \\cdot \\text{stringency deviation} \\right)\n\\]\nmade up of two components, \\(\\text{similarity deviation}\\) and \\(\\text{stringency deviation}\\), combined by constants \\(c_{1}\\) and \\(c_{2}\\) which are both \\(0.5\\) but can be altered to emphasize’s ones contribution over the other.\nThe \\(\\text{similarity deviation}\\) for a given strategy \\(i\\) is computed as the mean of the standard deviations of Cohen’s kappa scores \\(\\kappa_{ij}\\) for strategy \\(i\\) compared to another strategy \\(j\\) \\((j \\neq i)\\) across datasets. Here, \\(\\kappa_{ij}\\) is calculated by comparing the set of damaged cell labels given by strategy \\(i\\) to those given by strategy \\(j\\) for the same dataset. The standard deviation \\(\\sigma(\\kappa_{ij})\\) is taken across all datasets, and the mean is calculated over the all deviation scores for tool \\(i\\), amounting to \\({(J - 1)}\\) where \\(J\\) is the total number of strategies. Since \\(\\kappa\\) ranges from \\(-1\\) to \\(1\\), where \\(1\\) indicates complete agreement and \\(-1\\) indicates complete disagreement, deviation scores typically range on the lower end of \\((0, 1)\\). To standardize the similarity deviation score for the consistency equation, min-max scaling is applied across strategies, setting the highest mean deviation to \\(1\\) and the lowest to \\(0\\).\n\\[\n\\text{similarity deviation} = \\frac{1}{J - 1} \\sum_{j \\neq i} \\sigma(\\kappa_{ij})\n\\]\nMore simply, \\(\\text{stringency deviation}\\) is taken as the standard deviation in the stringency score for a particular strategy, \\(i\\), across datasets, where stringency is the percentage of damaged cells detected by the strategy. And as with \\(\\text{similarity deviation}\\), the \\(\\text{stringency deviation}\\) score is standardised by min-max scaling for the consistency equation, setting the highest deviation to \\(1\\) and the lowest to \\(0\\).\n\\[\n\\text{stringency deviation} =  \\sigma(\\text{damaged cells / total cells})\n\\]\n\nNote, each component is measured across the dataset types in isolation before being combined in the form of a median to generate the final deviation score. i.e, \\(\\text{stringency deviation}\\) is the median of the score taken across the groundtruth, non-groundtruth, and simulated datasets. This is done to avoid imbalancing the consistency score with any one dataset type.\n\nYou can view the isolated consistency scores, here.\n \n\n\nUsability\nThis is a manual and automated scoring of tool-based strategies according to 6 criteria,\n\n\n\n\n\n\n\n\n\n\nCriterion\nAutomated\n\nNo\nYes\n\n\n\n\nPresence of documentation\n\n0\n1\n1\n\n\nMore than one code deployment\n✓\n0\n1\n1\n\n\nLast adjustment to tool less than two years ago\n✓\n0\n1\n1\n\n\nAlterations to core code required\n\n1\n0\n1\n\n\nGitHub stars1\n✓\n0\n…\n5\n\n\nEase of use2\n\n…\n…\n5\n\n\n\nYou can view the isolated usability scores, here.\n\n\nTo add a new metric or suggest an adjustment to the above metrics, please visit the Contribute page."
  },
  {
    "objectID": "benchmark.html#footnotes",
    "href": "benchmark.html#footnotes",
    "title": "Benchmark",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGitHub stars underwent min-max scaling.↩︎\nSubjective score based on installation, dependencies, number of steps required to obtain output classification, accessibility of input data information, etc.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the Damage Control headquarters!",
    "section": "",
    "text": "Benchmark explore further descriptions of the benchmarking methods and metrics\nDatasets access the publicly available data used to perform the benchmark\nResults view the outcome of the benchmark\nContribute help us improve!\n\n\n\n\nFiltering cells that have lost transcriptomic information during isolation protocols, referred to here as “damaged cells”, is a fundamental quality control task for single cell RNA sequencing (scRNA-seq) data analysis. However, unlike other aspects of cell quality control, such as doublet and empty droplet removal, how to best filter damaged cells from single cell data has not been thoroughly explored.\nSearching online tutorials, forums, and published articles will bring you to the same conclusion: filter cells that express a high proportion of mitochondrial genes. This is because damaged cells are understood to have impaired membranes where cytoplasmic RNA content can escaped. It is for the same reason that the number of unique transcripts and UMIs per cell are also recommended metrics for identifying damaged cells.\nMore recently, the proportion of unspliced RNA, targeting transcripts that remain intact in the nucleus in the event of damage, and ribosomal RNA, targeting those that do not, are offered as damaged cell quality control metrics. Extending from this, the expression of the nuclear-located long non-coding RNA, MALAT1, has also been suggested as a potential metric for identifying damaged cells.\nThe problem for a user then arises in two forms: 1. Which cell quality controls metrics should I use and 2. According to what outlier detection framework should I perform filtering?\nDamaged cell detection tools provide their own solutions to the problems. However, the relative effectiveness of these tools, and of manual methods, is yet to be systematically determined.\nOur goal with this benchmark is to provide an objective answer to the question of how to best filter damaged cells in single cell data. Providing users with information on the characteristics of each available strategy, we aim for comprehensive, robust action towards addressing this quality control task."
  },
  {
    "objectID": "index.html#webpage-tour",
    "href": "index.html#webpage-tour",
    "title": "Welcome to the Damage Control headquarters!",
    "section": "",
    "text": "Benchmark explore further descriptions of the benchmarking methods and metrics\nDatasets access the publicly available data used to perform the benchmark\nResults view the outcome of the benchmark\nContribute help us improve!"
  },
  {
    "objectID": "index.html#motivation-for-the-benchmark",
    "href": "index.html#motivation-for-the-benchmark",
    "title": "Welcome to the Damage Control headquarters!",
    "section": "",
    "text": "Filtering cells that have lost transcriptomic information during isolation protocols, referred to here as “damaged cells”, is a fundamental quality control task for single cell RNA sequencing (scRNA-seq) data analysis. However, unlike other aspects of cell quality control, such as doublet and empty droplet removal, how to best filter damaged cells from single cell data has not been thoroughly explored.\nSearching online tutorials, forums, and published articles will bring you to the same conclusion: filter cells that express a high proportion of mitochondrial genes. This is because damaged cells are understood to have impaired membranes where cytoplasmic RNA content can escaped. It is for the same reason that the number of unique transcripts and UMIs per cell are also recommended metrics for identifying damaged cells.\nMore recently, the proportion of unspliced RNA, targeting transcripts that remain intact in the nucleus in the event of damage, and ribosomal RNA, targeting those that do not, are offered as damaged cell quality control metrics. Extending from this, the expression of the nuclear-located long non-coding RNA, MALAT1, has also been suggested as a potential metric for identifying damaged cells.\nThe problem for a user then arises in two forms: 1. Which cell quality controls metrics should I use and 2. According to what outlier detection framework should I perform filtering?\nDamaged cell detection tools provide their own solutions to the problems. However, the relative effectiveness of these tools, and of manual methods, is yet to be systematically determined.\nOur goal with this benchmark is to provide an objective answer to the question of how to best filter damaged cells in single cell data. Providing users with information on the characteristics of each available strategy, we aim for comprehensive, robust action towards addressing this quality control task."
  },
  {
    "objectID": "contribute.html",
    "href": "contribute.html",
    "title": "Contribute",
    "section": "",
    "text": "The results of our benchmark (Damage Bench, link) offer a static view of the current landscape of damaged cell detection. As new strategies emerge, and new ground truth datasets become available, we intend for them to be added to our benchmarking framework.\nIf you can offer a novel strategy, metric to benchmarking the strategies, or dataset, please think of contributing."
  },
  {
    "objectID": "stringency.html",
    "href": "stringency.html",
    "title": "Stringency",
    "section": "",
    "text": "Explore the current stringency results below or return to consensus.\n\n\n\n\n\n\nTo view the stringency scores for each of the 51 datasets, access here"
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "Explore the current consensus results below. The order of the strategies can be changed by clicking the arrows positioned by the column header of a metric, such as performance. For less cluttered viewing, select the columns you wish to see using the Column visibility button. Descriptions of each metric can be found here.\nTo explore isolated components of the consensus click here, performance, stringency, damage focus, consistency, usability"
  },
  {
    "objectID": "results.html#damaged-cell-detection-strategies",
    "href": "results.html#damaged-cell-detection-strategies",
    "title": "Results",
    "section": "Damaged Cell Detection Strategies",
    "text": "Damaged Cell Detection Strategies"
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Performance",
    "section": "",
    "text": "Explore the results below or return to consensus."
  },
  {
    "objectID": "performance.html#performance-on-ground-truth-datasets",
    "href": "performance.html#performance-on-ground-truth-datasets",
    "title": "Performance",
    "section": "Performance on ground truth datasets",
    "text": "Performance on ground truth datasets"
  },
  {
    "objectID": "performance.html#performance-on-simulated-datasets",
    "href": "performance.html#performance-on-simulated-datasets",
    "title": "Performance",
    "section": "Performance on simulated datasets",
    "text": "Performance on simulated datasets\n\nSimilarity of highly variable genes to the positive control\n\n\n\n\n\n\n\n\nDEG correctness relative to the positive control"
  },
  {
    "objectID": "consistency.html",
    "href": "consistency.html",
    "title": "Consistency",
    "section": "",
    "text": "Explore the current consistency results below or return to consensus."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Datasets",
    "section": "",
    "text": "All data used in the analysis is publicly available at the links provided below.\n\nGround truth data | 2 datasets, 5 damaged samples (*)\n\n\n\n\nDataset\nTreatment\nSorting\nSample\nAccession\nDownload\n\n\n\n\n\n\n\n\n\n\n\n\n\n*\nHEK293 cell line\nStaurosporine (1 μM, 2 hours)\nAnnexin V+ and DAPI+\nApoptotic\nPRJEB33078\nENA, BAM files\n\n\n*\nHEK293 cell line\nStaurosporine (1 μM, 2 hours)\nAnnexin V+ and DAPI-\nPro-apoptotic\nPRJEB33078\nENA, BAM files\n\n\n\nHEK293 cell line\n-\nAnnexin V- and DAPI-\nControl\nPRJEB33078\nENA, BAM files\n\n\n\n\n\n\n\n\n\n\n\n*\nGM18507 cell line\nTNF-alpha (100 ng/ml, 24 hours)\nPropidium iodide+ and Annexin V+\nDead\nEGAS00001003753\nZenodo, TENX049_SA928_003_sceset_v3_raw.rds\n\n\n*\nGM18507 cell line\nTNF-alpha (100 ng/ml, 24 hours)\nPropidium iodide- and Annexin V+\nDying\nEGAS00001003753\nZenodo, TENX049_SA928_002_sceset_v3_raw.rds\n\n\n\nGM18507 cell line\n-\nPropidium iodide- and Annexin V-\nControl\nEGAS00001003753\nZenodo, TENX049_SA928_001_sceset_v3_raw.rds\n\n\n\n\n\n\n\n\n\n\n\n*\nPDX\nTNF-alpha (100 ng/ml, 24 hours)\nPropidium iodide+ and Annexin V+\nDead\nEGAS00001003753\nZenodo, TENX019_SA604X7XB02089_004_sceset_v3_raw.rds\n\n\n\nPDX\n-\nPropidium iodide- and Annexin V-\nControl\nEGAS00001003753\nZenodo, TENX019_SA604X7XB02089_002_sceset_v3_raw.rds\n\n\n\n\n\nNon-ground truth data | 15 datasets, 15 samples\n\n\n\nDataset\nTissue Type\nAccession\nDownload\nData type\nProtocol\n\n\n\n\nHealthy Human tissue extracts\n\n\n\n\n\n\n\n4K PBMC (10x Genomics 2017)\nPBMC\nGSE108444\nLink\nFASTQ\n10x v2, normal\n\n\nHealthy Human Lung (Habermann, 2020)\nLung\nGSE135893\nLink1, Link2\nFASTQ\n10x v2, extra length\n\n\nLiver atlas (MacParland, 2018)\nLiver\nGSE115469\nLink\nBAM to FASTQ\n10x v2, normal\n\n\n\n\n\n\n\n\n\n\nDiseased Human tissue extracts\n\n\n\n\n\n\n\nTB PBMC (Cai, 2020)\nPBMC\nPRJNA605083\nLink1, Link2\nFASTQ\n10x v2, normal\n\n\nTB Lung (Wang, 2023)\nLung\nGSE192483\nLink1, Link2\nFASTQ\n10x v2, extra length\n\n\nHBV Liver (Beudeker, 2024)\nLiver\nGSE247322\nLink\nSRA to FASTQ\n10x v3\n\n\n\n\n\n\n\n\n\n\nMouse tissue extracts\n\n\n\n\n\n\n\n10K Mouse PBMC (10X Genomics, 2021)\nPBMC\n10X Genomics\nLink\nFASTQ\n10x v3\n\n\nMouse lung (Li, 2021)\nLung\n10X Genomics\nLink1, Link2\nFASTQ\n10x v3\n\n\nLiver necroposis (Vucur, 2023)\nLiver\nGSE166875\nLink1, Link2\nFASTQ\n10x v2, normal\n\n\n\n\n\n\n\n\n\n\nHuman tumours\n\n\n\n\n\n\n\nHuman Hodgkin’s Lymphoma (10x Genomics 2020b)\nLymphoid tissue\n10X Genomics\nLink\nFASTQ\n10x v3\n\n\nHuman Glioblastoma (10x Genomics 2020)\nGlioblastoma\n10X Genomics\nLink\nFASTQ\n10x v3\n\n\nHuman Ductal Carcinoma (10x Genomics 2021)\nBreast cancer tissue\n10X Genomics\nLink\nFASTQ\n10x v3\n\n\n\n\n\n\n\n\n\n\nHuman cell lines\n\n\n\n\n\n\n\n5k A549, Lung Carcinoma Cells (10X Genomics 2021)\n-\nGSE161363\nLink\nFASTQ\n10x v3\n\n\nRaji and Jurkat Cells (10X Genomics, 2021)\n-\n10X Genomics\nLink\nFASTQ\n10x v3\n\n\nHCT-116 (Meir, 2016)\n-\nGSE144357\nLink1, Link2\nFASTQ\n10x v2, normal\n\n\n\n\n\nSimulated data\n1 parent dataset, 30 final samples\nAll simulated samples were created from a dataset available in R through the SeuratData package. The interferon-treated PBMC dataset was selected as it contains case and control data already intergrated into a single object with cell annotations.\n# IFNB-Stimulated & Control PBMC data\nlibrary(SeuratData)\nInstallData(\"ifnb\")\ndata(\"ifnb\")\nifnb &lt;- UpdateSeuratObject(ifnb)\nThese foundational simulated samples were altered by randomly extracting a fixed percentage of their cells (2.5, 5, 10, 15, or 20), passing them through a damage-pertubration function, and reintroducing them to the sample. The resulting datasets are available for download on Zenodo, ."
  }
]